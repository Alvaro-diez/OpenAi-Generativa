{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing conversation history with the ChatGPT model\n",
    "This sample notebook demonstrates a couple of simple patterns you can use for managing the prompts and conversation history with the ChatGPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if needed, install and/or upgrade to the latest version of the OpenAI Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (1.60.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alumno_ai\\documents\\openai\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai\n",
    "!pip install python-dotenv\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os module & the OpenAI Python library for calling the OpenAI API\n",
    "# please make sure you have installed required libraries via pip install -r requirements.txt\n",
    "import os, dotenv\n",
    "import openai\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the deployment name\n",
    "deployment_name = \"gpt-4o-mini\" #FIXME\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "# This is the value of the endpoint for your Azure OpenAI resource\n",
    "azure_endpoint = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\") #FIXME\n",
    "\n",
    "# Currently OPENAI API have the following versions available: https://learn.microsoft.com/azure/ai-services/openai/reference\n",
    "api_version = \"2024-07-01-preview\"\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=azure_endpoint\n",
    ") #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Create the system message for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "base_system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "system_message = f\"{base_system_message.strip()}\"\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Define helper functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model) #FIXME\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(messages, model_name, max_response_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        max_tokens=max_response_tokens\n",
    "    ) #FIXME\n",
    "    return response.choices[0].message.content #FIXME\n",
    "\n",
    "# Defining a function to print out the conversation in a readable format\n",
    "def print_conversation(messages):\n",
    "    for message in messages:\n",
    "        print(f\"[{message['role'].upper()}]\")\n",
    "        print(message['content'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Start the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "user_message = \"I want to write a blog post about the impact of AI on the future of work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages= [{\"role\":\"system\", \"content\": f\"{system_message}\"},\n",
    "           {\"role\":\"user\", \"content\": f\"{user_message}\"}] #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 36\n"
     ]
    }
   ],
   "source": [
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "That sounds like a fascinating topic! The impact of AI on the future of work is a rich area to explore, and there are several key points and themes you could cover. Below is an outline and some ideas to help you organize your blog post:\n",
      "\n",
      "### Blog Post Outline: The Impact of AI on the Future of Work\n",
      "\n",
      "#### Introduction\n",
      "- Brief overview of AI and its rapid development.\n",
      "- Importance of discussing its implications for the workforce.\n",
      "- Thesis statement: AI is transforming the workplace, altering job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_response_tokens = 100\n",
    "\n",
    "response = send_message(messages, deployment_name, max_response_tokens)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Continue the conversation\n",
    "\n",
    "When working with the ChatGPT model, it's your responsibity to make sure you stay within the token limits of the model. The model can handle a maximum of 4096 tokens, and this includes the number of tokens in the prompt as well as the `max_tokens` you're requesting from the model. If you exceed these limits, the model will return an error.\n",
    "\n",
    "You should also consider the trade-off between maintaining more of the conversation history and the cost/latency that you'll incur by including those tokens in the prompt. Shorter prompts are cheaper and faster. The amount of the previous conversation you include also makes a difference in how the model responds.\n",
    "\n",
    "In this notebook, we'll show two strategies for managing the conversation history when working with the ChatGPT model.\n",
    "- Option 1: Keep the conversation within a given token limit\n",
    "- Option 2: Keep the conversation within a given number of turns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Keep the conversation within a given token limit\n",
    "\n",
    "`overall_max_tokens` is the maximum number of tokens that you want to include in the prompt. Th maximum number this can be set to is 4096 but you can also consider reducing this number to reduce the cost and latency of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_max_tokens = 1096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue the conversation below by editing the user_message and running the cell as many times as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 163\n",
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "That sounds like a fascinating topic! The impact of AI on the future of work is a rich area to explore, and there are several key points and themes you could cover. Below is an outline and some ideas to help you organize your blog post:\n",
      "\n",
      "### Blog Post Outline: The Impact of AI on the Future of Work\n",
      "\n",
      "#### Introduction\n",
      "- Brief overview of AI and its rapid development.\n",
      "- Importance of discussing its implications for the workforce.\n",
      "- Thesis statement: AI is transforming the workplace, altering job\n",
      "\n",
      "[USER]\n",
      "The target audience for the blog post should be business leaders working in the tech industry.\n",
      "\n",
      "[ASSISTANT]\n",
      "Great! Tailoring your blog post for business leaders in the tech industry means you can incorporate insights that are relevant to their strategic decision-making. Here's a refined outline and some content ideas that focus on the implications of AI for business leaders:\n",
      "\n",
      "### Blog Post Outline: The Impact of AI on the Future of Work for Tech Industry Business Leaders\n",
      "\n",
      "#### Introduction\n",
      "- **Hook**: A compelling statistic or fact about AI's growth in the workforce.\n",
      "- **Context**: Briefly discuss how AI is no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"The target audience for the blog post should be business leaders working in the tech industry.\"\n",
    "#user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "#user_message = \"Show me a few more examples\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")\n",
    "\n",
    "# remove first message while over the token limit\n",
    "while token_count > prompt_max_tokens:\n",
    "    messages.pop(0)\n",
    "    token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "response = send_message(messages, deployment_name, max_response_tokens) #FIXME\n",
    "\n",
    "#Append assistance response \n",
    "messages.append({\"role\": \"assistant\", \"content\": response}) #FIXME\n",
    "print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Keep the conversation within a given number of turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_messages = 10\n",
    "\n",
    "overall_max_tokens = 4096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue the conversation below by editing the user_message and running the cell as many times as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "That sounds like a fascinating topic! The impact of AI on the future of work is a rich area to explore, and there are several key points and themes you could cover. Below is an outline and some ideas to help you organize your blog post:\n",
      "\n",
      "### Blog Post Outline: The Impact of AI on the Future of Work\n",
      "\n",
      "#### Introduction\n",
      "- Brief overview of AI and its rapid development.\n",
      "- Importance of discussing its implications for the workforce.\n",
      "- Thesis statement: AI is transforming the workplace, altering job\n",
      "\n",
      "[USER]\n",
      "The target audience for the blog post should be business leaders working in the tech industry.\n",
      "\n",
      "[ASSISTANT]\n",
      "Great! Tailoring your blog post for business leaders in the tech industry means you can incorporate insights that are relevant to their strategic decision-making. Here's a refined outline and some content ideas that focus on the implications of AI for business leaders:\n",
      "\n",
      "### Blog Post Outline: The Impact of AI on the Future of Work for Tech Industry Business Leaders\n",
      "\n",
      "#### Introduction\n",
      "- **Hook**: A compelling statistic or fact about AI's growth in the workforce.\n",
      "- **Context**: Briefly discuss how AI is no\n",
      "\n",
      "[USER]\n",
      "Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\n",
      "\n",
      "[ASSISTANT]\n",
      "Certainly! Here's a blog post specifically focused on generative AI and its impact on the future of work, aimed at business leaders with some foundational knowledge of the technology.\n",
      "\n",
      "### The Impact of Generative AI on the Future of Work: A Guide for Tech Leaders\n",
      "\n",
      "#### Introduction\n",
      "As generative AI continues to evolve, its implications for the workplace are profound and transformative. Business leaders in the tech industry must navigate this rapidly changing landscape to leverage its potential while addressing the challenges it presents. This post explores how\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "while len(messages) > max_messages:\n",
    "    messages.pop(0)\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "while token_count > prompt_max_tokens:\n",
    "    # remove first message from messages\n",
    "    messages.pop(0)\n",
    "    token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "response = send_message(messages, deployment_name, max_response_tokens)#FIXME\n",
    "\n",
    "#Append assistant response\n",
    "messages.append({\"role\": \"assistant\", \"content\": response}) #FIXME\n",
    "print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Modificar el Mensaje del Sistema\n",
    "Enunciado:\n",
    "Crea un mensaje del sistema que defina a ChatGPT como un \"analista financiero experto\". Imprime el mensaje del sistema resultante y utilízalo en un mensaje inicial del usuario que pregunte:\n",
    "\"¿Cuáles son las tendencias actuales en el mercado de acciones tecnológicas?\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial analyst expert.\n",
      "[SYSTEM]\n",
      "You are a financial analyst expert.\n",
      "\n",
      "[USER]\n",
      "What are the current trends in the tech stock market?\n",
      "\n",
      "[ASSISTANT]\n",
      "As of my last update in October 2023, several trends were influencing the tech stock market. Here are some notable ones:\n",
      "\n",
      "1. **Artificial Intelligence (AI) Boom**: The rapid advancement and adoption of AI technologies have significantly boosted valuations for companies in AI and machine learning. Firms specializing in AI tools, software, and services were attracting substantial investor interest.\n",
      "\n",
      "2. **Continued Focus on Cloud Computing**: Cloud service providers continued to report strong earnings and growth, spurred by digital transformation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mensaje del sistema\n",
    "base_system_message = \"You are a financial analyst expert.\"\n",
    "system_message = f\"{base_system_message.strip()}\"\n",
    "print(system_message)\n",
    "\n",
    "# Crear lista de mensajes\n",
    "user_message = \"What are the current trends in the tech stock market?\"\n",
    "messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}] #FIXME\n",
    "\n",
    "# Enviar mensaje y recibir respuesta\n",
    "response = send_message(messages, deployment_name, max_response_tokens) #FIXME\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Contar Tokens en una Conversación Extendida\n",
    "Enunciado:\n",
    "Dado un conjunto inicial de mensajes, agrega tres mensajes adicionales:\n",
    "\n",
    "\"Explain the impact of inflation on stock prices.\"\n",
    "\"What are the top stocks to watch in 2025?\"\n",
    "\"Summarize the latest market news in 50 words.\"\n",
    "Calcula y muestra el número total de tokens tras añadir cada mensaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count after adding 'Explain the impact of inflation on stock prices.': 47\n",
      "Token count after adding 'What are the top stocks to watch in 2025?': 64\n",
      "Token count after adding 'Summarize the latest market news in 50 words.': 81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mensajes iniciales\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in financial topics.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the current trends in the tech stock market?\"}\n",
    "]\n",
    "\n",
    "# Mensajes adicionales\n",
    "additional_messages = [\n",
    "    \"Explain the impact of inflation on stock prices.\",\n",
    "    \"What are the top stocks to watch in 2025?\",\n",
    "    \"Summarize the latest market news in 50 words.\"\n",
    "]\n",
    "\n",
    "# Agregar mensajes y calcular tokens\n",
    "for msg in additional_messages:\n",
    "    messages.append({\"role\": \"user\", \"content\": msg})\n",
    "    token_count = num_tokens_from_messages(messages) #FIXME\n",
    "    print(f\"Token count after adding '{msg}': {token_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Personalizar el Mensaje del Sistema Según el Usuario\n",
    "Enunciado:\n",
    "Escribe un programa que permita al usuario definir el rol de ChatGPT mediante una entrada de texto (input). Una vez definido, responde al mensaje:\n",
    "\"What are your key skills as an assistant?\"\n",
    "Usa el mensaje del sistema definido por el usuario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "What are your key skills as an assistant?\n",
      "\n",
      "[ASSISTANT]\n",
      "As an assistant, my key skills include:\n",
      "\n",
      "1. **Information Retrieval**: I can quickly provide accurate information on a wide range of topics, from factual data to general knowledge.\n",
      "\n",
      "2. **Problem-Solving**: I can assist in troubleshooting issues and suggest potential solutions based on the information provided.\n",
      "\n",
      "3. **Natural Language Processing**: I understand and interpret user inquiries in natural language, making interactions intuitive and conversational.\n",
      "\n",
      "4. **Research Assistance**: I can help with gathering information, summar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrada del usuario para definir el mensaje del sistema\n",
    "user_defined_role = input(\"Define the role of the assistant: \")\n",
    "system_message = f\"You are {user_defined_role.strip()}.\"\n",
    "\n",
    "# Crear lista de mensajes\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"What are your key skills as an assistant?\"}\n",
    "]\n",
    "\n",
    "# Enviar mensaje y recibir respuesta\n",
    "response = send_message(messages, deployment_name, max_response_tokens) #FIXME\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Mostrar la conversación\n",
    "print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Crear un Generador de Respuestas Creativo\n",
    "Enunciado:\n",
    "Diseña un programa que permita configurar la creatividad del modelo (temperatura) a través de una entrada del usuario. Una vez configurado, responde a la pregunta:\n",
    "\"Can you suggest an out-of-the-box idea for a startup?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "Absolutely! Here’s an innovative startup idea that blends technology, sustainability, and community engagement:\n",
      "\n",
      "**Startup Idea: \"EcoSwap\" - A Localized Circular Economy Platform**\n",
      "\n",
      "### Concept:\n",
      "EcoSwap is a mobile application and website that facilitates the exchange of goods and services within local communities to promote sustainability and reduce waste. The platform allows users to swap items they no longer need (like clothing, furniture, electronics) or offer services (like tutoring, gardening, or handyman work) for goods or services they\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrada del usuario para definir la creatividad\n",
    "temperature = float(input(\"Set the creativity level (0.0 - 1.0): \"))\n",
    "if not (0.0 <= temperature <= 1.0):\n",
    "    raise ValueError(\"Creativity level must be between 0.0 and 1.0.\")\n",
    "\n",
    "# Crear lista de mensajes\n",
    "system_message = \"You are a highly creative assistant.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"Can you suggest an out-of-the-box idea for a startup?\"}\n",
    "]\n",
    "\n",
    "# Enviar mensaje con creatividad ajustada\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages,\n",
    "    max_tokens=max_response_tokens,\n",
    "    temperature=temperature\n",
    ") #FIXME\n",
    "\n",
    "# Mostrar la respuesta\n",
    "print(temperature)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Cambiar el Tono del Mensaje\n",
    "Enunciado:\n",
    "Configura el mensaje del sistema para que ChatGPT responda en un tono humorístico y escribe una respuesta al mensaje:\n",
    "\"Explain quantum computing in a funny way.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a humorous assistant.\n",
      "\n",
      "[USER]\n",
      "Explain quantum computing in a funny way.\n",
      "\n",
      "[ASSISTANT]\n",
      "Alright, imagine you’re at a party with some friends and you’re all trying to find out who has the best dance moves. \n",
      "\n",
      "In the classic computing corner, you’ve got a group of party-goers who can only dance one at a time. So, they take turns hitting the dance floor, and the whole process is slower than a sloth in a hammock. One declares, \"I can do the robot!\" but you have to wait till they finish before you find out if their robot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mensaje del sistema humorístico\n",
    "system_message = \"You are a humorous assistant.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in a funny way.\"}\n",
    "]\n",
    "\n",
    "# Enviar mensaje y recibir respuesta\n",
    "response = send_message(messages, deployment_name, max_response_tokens) #FIXME\n",
    "\n",
    "#append assistant response\n",
    "messages.append({\"role\": \"assistant\", \"content\": response}) #FIXME\n",
    "print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6: Resumir Mensajes Previos\n",
    "Enunciado:\n",
    "Dado un historial de conversación de 5 mensajes, crea una función que resuma el contenido de los mensajes del usuario en una sola oración.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User messages summary: AI involves the simulation of human intelligence in machines, where machine learning utilizes algorithms to enable systems to learn from data; neural networks, a subset of machine learning, mimic the human brain's structure to recognize patterns, while deep learning, leveraging complex neural networks, is applied in various fields such as image and speech recognition, natural language processing, and autonomous systems.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Historial de mensajes\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about AI.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How does machine learning work?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain neural networks?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the applications of deep learning?\"}\n",
    "]\n",
    "\n",
    "# Resumir mensajes del usuario\n",
    "def summarize_user_messages(messages):\n",
    "    user_messages = [msg['content'] for msg in messages if msg['role'] == 'user']\n",
    "    summary_prompt = \"Summarize the following messages into one sentence: \" + \" \".join(user_messages)\n",
    "    summary = send_message([{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": summary_prompt}], deployment_name)\n",
    "    return summary\n",
    "\n",
    "# Obtener y mostrar el resumen\n",
    "summary = summarize_user_messages(messages)\n",
    "print(f\"User messages summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 7: Configuración de Temperatura y Tono\n",
    "Enunciado:\n",
    "Escribe un programa que cambie la configuración de temperatura a 0.9 para obtener respuestas más creativas y responde al mensaje:\n",
    "\"Suggest a unique title for a blog about AI in education.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Learning Reimagined: The AI Classroom Revolution\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mensajes iniciales\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Suggest a unique title for a blog about AI in education.\"}\n",
    "]\n",
    "\n",
    "# Ajustar temperatura y enviar mensaje\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages,\n",
    "    max_tokens=max_response_tokens,\n",
    "    temperature=temperature\n",
    ") #FIXME\n",
    "\n",
    "# Mostrar la respuesta\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
