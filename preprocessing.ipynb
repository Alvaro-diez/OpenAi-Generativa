{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning\n",
    "\n",
    "Data cleaning is an essential step in the process of preparing data for further processing and serving into a large language model. Without data cleaning, the language model would be limited in its ability to interpret and process the data accurately. Data cleaning helps to ensure that the data is in a consistent format, free of any inconsistencies or errors that could lead to incorrect results. It also helps to reduce the amount of time needed to process the data, as it eliminates the need to manually check for any inconsistencies or errors. Additionally, data cleaning can help to improve the accuracy of the language model by removing any irrelevant or incorrect data. By taking the time to properly clean the data, the language model can more accurately interpret and process the data, leading to better results.\n",
    "\n",
    ">  Fill out the missing pieces in the source source to get everything working (indicated by `#FIXME`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count tokens\n",
    "\n",
    "> **Important note**\n",
    ">\n",
    "> *You do not need to manually tokenize strings before feeding texts into the model. This will be done automatically once you put instructions into the `prompt` parameter. However, you can use the `tiktoken` library to check how a string is tokenized and count the numbers of tokens to calculate the cost of an API call. Learn more [here](https://platform.openai.com/docs/introduction/tokens).*\n",
    "\n",
    "Tokenizing text strings is an important step in natural language processing (NLP) as it helps models like GPT-3 understand the structure of a text string. Tokenizing breaks a text string into smaller pieces called tokens, which can then be analyzed and used by the model. By understanding the structure of a text string, models can better understand the meaning of the text. Additionally, tokenizing helps to determine the cost of an Azure OpenAI Service API call, as usage is priced by token. Furthermore, different models use different encodings, so it is important to tokenize text strings in the appropriate format.\n",
    "\n",
    "`tiktoken` supports three encodings used by Azure OpenAI Service models:\n",
    "\n",
    "| Encoding name | Azure OpenAI Service models |\n",
    "| ------------- | -------------- |\n",
    "| gpt2 (or r50k_base) | Most GPT-3 models |\n",
    "| p50k_base | Code models, text-davinci-002, text-davinci-003 |\n",
    "| cl100k_base | text-embedding-ada-002 |\n",
    "\n",
    "Tokens in English typically range from one character to one word (e.g. \"t\" or \"great\"), though some languages may have tokens that are shorter or longer than one character or word. Spaces are usually placed at the beginning of words (e.g. \" is\" instead of \"is \" or \"+\"is\"). You can use the Tokenizer to quickly check how a string is tokenized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show it briefly, we will use `tiktoken` to tokenize a text string and see how the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-win_amd64.whl (883 kB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 995, 11, 428, 318, 1257, 0]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "encoding.encode(\"Hello world, this is fun!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 995, 11, 428, 318, 1257, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "encoding.encode(\"Hello world, this is fun!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a script that shows the string tokens from an input phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 11106, 8358, 3305, 1556, 292]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FIXME\n",
    "palabra = \"Holi que tal estas\"\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "encoding.encode(palabra)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to count the number of tokens in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[15496, 2159, 11, 428, 318, 1257, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_num_tokens_from_string(string: str, encoding_name: str='p50k_base') -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    encoded_tokens = encoding.encode(string)\n",
    "    return len(encoded_tokens)\n",
    "\n",
    "\n",
    "print(get_num_tokens_from_string(\"Hello World, this is fun!\"))\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "encoded_tokens = encoding.encode(\"Hello World, this is fun!\")\n",
    "print(encoded_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll perform some light data cleaning by removing redundant whitespace and cleaning up the punctuation to prepare the data for tokenization. Again, tokenization is not required for the model to work, but it is a good practice to do so to ensure that the data is in a consistent format and that the model is able to process the data correctly. Also, it makes sure that the request is not too long for the model as the maximum number of tokens for davinci is 2048, e.g., equivalent to around 2-3 pages of text.\n",
    "\n",
    "> **Best Practices**\n",
    "> - **Replace newlines with a single space**: Unless you're embedding code, we suggest replacing newlines (\\n) in your input with a single space, as we have observed inferior results when newlines are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(string, sep_token = \" \\n \"):\n",
    "    \"\"\"Normalize text by removing unnecessary characters and altering the format of words.\"\"\"\n",
    "    # make text lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    string = re.sub(r'\\s+',  ' ', string).strip()\n",
    "    string = re.sub(r\". ,\",\"\",string)\n",
    "    # remove all instances of multiple spaces\n",
    "    string = string.replace(\"..\",\".\")\n",
    "    string = string.replace(\". .\",\".\")\n",
    "    string = string.replace(\"\\n\", \"\")\n",
    "    string = string.strip()\n",
    "    return string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data to test the cleaning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lorem = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor             \\n incididunt ut labore et dolore magna aliqua.\\n \" \\\n",
    "        \"Ut enim ad minim veniam, \\n quis nostrud          exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\\n \" \\\n",
    "        \"Duis aute irure dolor in reprehenderit in VOLUPTATE \\n velit esse cillum dolore           eu fugiat nulla pariatur.\\n\" \\\n",
    "        \"Excepteur sint occaecat cupidatat non proident, \\n       sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "\n",
    "normalize_text(lorem) #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Validar longitud de texto antes de enviar a OpenAI\n",
    "\n",
    "Escribe una funci贸n llamada validate_text_length que valide si un texto supera un l铆mite de tokens especificado. Si el texto supera el l铆mite, la funci贸n debe dividirlo en segmentos de tama帽o adecuado para enviarlos al modelo de OpenAI. Usa la codificaci贸n cl100k_base.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de segmentos:  8\n",
      "Numero de tokens en el segmento:  2048\n",
      "[33883, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439, 48031, 27439]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def validate_text_length(text, max_tokens=2048, encoding_name='cl100k_base'):\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    encoded_tokens = encoding.encode(text)\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start < len(encoded_tokens):\n",
    "        end = start + max_tokens\n",
    "        segment = encoded_tokens[start:end]\n",
    "        segments.append(encoding.decode(segment))\n",
    "        start = end\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Prueba la funci贸n\n",
    "text = \"Lorem ipsum \" * 8000  # Texto largo\n",
    "segments = validate_text_length(text)\n",
    "\n",
    "# Verifica el n煤mero de segmentos y su longitud\n",
    "print(f\"Numero de segmentos: \", len(segments))  # Resultado esperado: M谩s de 1 segmento\n",
    "print(f\"Numero de tokens en el segmento: \", len(tiktoken.get_encoding('cl100k_base').encode(segments[0])))  # <= 2048 tokens\n",
    "print(tiktoken.get_encoding('cl100k_base').encode(segments[0]))\n",
    "##Resultado esperado:\n",
    "\n",
    "##El texto se divide en segmentos adecuados para ser procesados por OpenAI.\n",
    "##Cada segmento tendr谩 un n煤mero de tokens menor o igual al l铆mite especificado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Filtrar palabras prohibidas en un texto\n",
    "\n",
    "Escribe una funci贸n llamada filter_prohibited_words que remueva palabras prohibidas de un texto antes de enviarlo a OpenAI. \n",
    "\n",
    "Como ejemplo queremos Eliminar palabras como: ['password', 'confidential', 'secret']. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document contains [REDACTED] and [REDACTED] information.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def filter_prohibited_words(text, prohibited_words):\n",
    "    #FIXME\n",
    "    texto = text.split()\n",
    "    for i, p in enumerate(texto):\n",
    "        if p in prohibited_words:\n",
    "            texto[i] = texto[i].replace(p, \"[REDACTED]\")\n",
    "    return \" \".join(texto)\n",
    "\n",
    "# Prueba la funci贸n\n",
    "text = \"This document contains confidential and secret information.\"\n",
    "prohibited_words = ['password', 'confidential', 'secret']\n",
    "\n",
    "cleaned_text = filter_prohibited_words(text, prohibited_words) #FIXME\n",
    "print(cleaned_text)  # Resultado esperado: \"This document contains [REDACTED] and [REDACTED] information.\"\n",
    "#Resultado esperado:\n",
    "#El texto tendr谩 palabras prohibidas reemplazadas por [REDACTED].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Identificar idioma del texto\n",
    "\n",
    "Crea una funci贸n detect_language que use langdetect para identificar el idioma de un texto y devuelva el c贸digo del idioma (e.g., en, es). Verifica el idioma antes de enviarlo al modelo OpenAI. Debes instalar la libreria langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---- --------------------------------- 112.6/981.5 kB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 655.4/981.5 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 6.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993255 sha256=e5a90566fe49d2198de9dd5d4d5d52caf64a0114f95c1836b682791d8152f135\n",
      "  Stored in directory: c:\\users\\alumno_ai\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detecta el idioma de un texto.\"\"\"\n",
    "    #FIXME\n",
    "    return detect(text)\n",
    "\n",
    "# Prueba la funci贸n\n",
    "text = \"Hola, este es un texto en espa帽ol.\"\n",
    "text2 = \"Hello, this text is in english\"\n",
    "language = detect_language(text) #FIXME\n",
    "language2 = detect_language(text2)\n",
    "print(language)  # Resultado esperado: 'es' (para espa帽ol)\n",
    "print(language2)\n",
    "##Resultado esperado:\n",
    "##El idioma del texto detectado (es, en, etc.) ser谩 devuelto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Generar prompts efectivos\n",
    "\n",
    "Escribe una funci贸n generate_effective_prompt que genere un prompt optimizado para OpenAI, dado un tema y un objetivo. Usa plantillas como:\n",
    "Explain [tema] in simple terms for a [audiencia].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain machine learning in simple terms for a general audience.\n"
     ]
    }
   ],
   "source": [
    "def generate_effective_prompt(topic, audience=\"general audience\"):\n",
    "    \"\"\"Genera un prompt optimizado para OpenAI.\"\"\"\n",
    "    #FIXME\n",
    "    prompt = f\"Explain {topic} in simple terms for a {audience}.\"\n",
    "    return prompt\n",
    "\n",
    "# Prueba la funci贸n\n",
    "topic = \"machine learning\"\n",
    "prompt = generate_effective_prompt(topic)  #FIXME \n",
    "print(prompt)  # Resultado esperado: \"Explain machine learning in simple terms for a beginner.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6: Medir costo estimado del uso de OpenAI\n",
    "\n",
    "Crea una funci贸n estimate_cost que calcule el costo estimado de una solicitud a OpenAI basado en el n煤mero de tokens. Sup贸n un costo de $0.02 por 1000 tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10002000000000001\n"
     ]
    }
   ],
   "source": [
    "def estimate_cost(text, encoding_name='cl100k_base', cost_per_1k_tokens=0.02):\n",
    "    \"\"\"Calcula el costo estimado de un texto basado en el n煤mero de tokens.\"\"\"\n",
    "    #FIXME\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    encoded_tokens = encoding.encode(text)\n",
    "    cost1k = len(encoded_tokens) / 1000\n",
    "    cost = cost1k * cost_per_1k_tokens\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Prueba la funci贸n\n",
    "text = \"This is a sample text.\" * 1000\n",
    "cost = estimate_cost(text)\n",
    "print(cost)  # Resultado esperado: Costo aproximado basado en la cantidad de tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 7: Limpiar texto JSON para OpenAI\n",
    "\n",
    "Escribe una funci贸n clean_json_text que tome un JSON como entrada y limpie todas las claves y valores de caracteres no ASCII.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key1': 'Hello ', 'key2': 'Caf and th'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_value(value):\n",
    "    \"\"\"Elimina caracteres no ASCII de un valor.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return re.sub(r'[^\\x00-\\x7F]+', '', value)\n",
    "    return value\n",
    "\n",
    "def clean_json_text(json_obj):\n",
    "    \"\"\"Limpia texto JSON eliminando caracteres no ASCII.\"\"\"\n",
    "    return {k: clean_value(v) for k, v in json_obj.items()}\n",
    "\n",
    "# Prueba la funci贸n\n",
    "data = {\"key1\": \"Hello \", \"key2\": \"Caf茅 and th茅\"}\n",
    "cleaned_data = clean_json_text(data)\n",
    "print(cleaned_data)  # Resultado esperado: {\"key1\": \"Hello \", \"key2\": \"Caf and th\"}\n",
    "##Resultado esperado:\n",
    "##Un JSON sin caracteres no ASCII."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
