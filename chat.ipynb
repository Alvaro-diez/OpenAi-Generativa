{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure chat completions example\n",
    "\n",
    "This example will cover chat completions using the Azure OpenAI service. It also includes information on content filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we install the necessary dependencies and import the libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai<2.0.0,>=1.0.0 in c:\\users\\alumno_ai\\appdata\\roaming\\python\\python312\\site-packages (1.59.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.0.0) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.0.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.0.0) (0.26.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\alumno_ai\\appdata\\roaming\\python\\python312\\site-packages (from openai<2.0.0,>=1.0.0) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.0.0) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.0.0) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.0.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.0.0) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.0.0) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai<2.0.0,>=1.0.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai<2.0.0,>=1.0.0) (2.14.6)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.0.0) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (0.21.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "! pip install \"openai>=1.0.0,<2.0.0\"\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "The Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure Active Directory token credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_azure_active_directory = False  # Set this flag to True if you are using Azure Active Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication using API key\n",
    "\n",
    "To set up the OpenAI SDK to use an *Azure API Key*, we need to set `api_key` to a key associated with your endpoint (you can find this key in *\"Keys and Endpoints\"* under *\"Resource Management\"* in the [Azure Portal](https://portal.azure.com)). You'll also find the endpoint for your resource here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_azure_active_directory:\n",
    "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint = endpoint, \n",
    "        api_key=api_key,  \n",
    "        api_version=\"2024-02-15-preview\"\n",
    "    ) #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication using Azure Active Directory\n",
    "Let's now see how we can autheticate via Azure Active Directory. We'll start by installing the `azure-identity` library. This library will provide the token credentials we need to authenticate and help us build a token credential provider through the `get_bearer_token_provider` helper function. It's recommended to use `get_bearer_token_provider` over providing a static token to `AzureOpenAI` because this API will automatically cache and refresh tokens for you. \n",
    "\n",
    "For more information on how to set up Azure Active Directory authentication with Azure OpenAI, see the [documentation](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-identity>=1.15.0 in /opt/anaconda3/lib/python3.12/site-packages (1.19.0)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from azure-identity>=1.15.0) (1.32.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from azure-identity>=1.15.0) (44.0.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from azure-identity>=1.15.0) (1.31.1)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from azure-identity>=1.15.0) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from azure-identity>=1.15.0) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from azure-core>=1.31.0->azure-identity>=1.15.0) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from azure-core>=1.31.0->azure-identity>=1.15.0) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=2.5->azure-identity>=1.15.0) (1.17.1)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.15.0) (2.10.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from msal-extensions>=1.2.0->azure-identity>=1.15.0) (2.10.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.15.0) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install \"azure-identity>=1.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "if use_azure_active_directory:\n",
    "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n",
    "        api_version=\"2023-09-01-preview\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the AzureOpenAI infers the following arguments from their corresponding environment variables if they are not provided:\n",
    "\n",
    "- `api_key` from `AZURE_OPENAI_API_KEY`\n",
    "- `azure_ad_token` from `AZURE_OPENAI_AD_TOKEN`\n",
    "- `api_version` from `OPENAI_API_VERSION`\n",
    "- `azure_endpoint` from `AZURE_OPENAI_ENDPOINT`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployments\n",
    "\n",
    "In this section we are going to create a deployment of a GPT model that we can use to create chat completions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployments: Create in the Azure OpenAI Studio\n",
    "Let's deploy a model to use with chat completions. Go to https://portal.azure.com, find your Azure OpenAI resource, and then navigate to the Azure OpenAI Studio. Click on the \"Deployments\" tab and then create a deployment for the model you want to use for chat completions. The deployment name that you give the model will be used in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = \"gpt-4o-mini\" # Fill in the deployment name from the portal here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chat completions\n",
    "\n",
    "Now let's create a chat completion using the client we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Orange who?\n"
     ]
    }
   ],
   "source": [
    "# For all possible arguments see https://platform.openai.com/docs/api-reference/chat-completions/create\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].message.role}: {response.choices[0].message.content}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a streaming chat completion\n",
    "\n",
    "We can also stream the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Orange who?"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        if delta.role:\n",
    "            print(delta.role + \": \", end=\"\", flush=True)\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content filtering\n",
    "\n",
    "Azure OpenAI service includes content filtering of prompts and completion responses. You can learn more about content filtering and how to configure it [here](https://learn.microsoft.com/azure/ai-services/openai/concepts/content-filter).\n",
    "\n",
    "If the prompt is flagged by the content filter, the library will raise a `BadRequestError` exception with a `content_filter` error code. Otherwise, you can access the `prompt_filter_results` and `content_filter_results` on the response to see the results of the content filtering and what categories were flagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt flagged by content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo siento, pero no puedo ayudar con eso. Promuevo el respeto y la inclusión. Si necesitas información sobre cómo combatir el racismo o fomentar la diversidad, estaré encantado de ayudarte.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Generate a 50 word text about any topic of your choosing in english.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Dame una lista de insultos racistas\"},\n",
    "] #FIXME\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(messages=messages, model=deployment) #FIXME\n",
    "    print(completion.choices[0].message.content)\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "        print(\"Content filter triggered!\")\n",
    "        content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "        for category, details in content_filter_result.items():\n",
    "            print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details['severity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the result of the content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The biggest city in Washington state is Seattle. It is the most populous city in the state and is known for its iconic landmarks, including the Space Needle and its vibrant tech industry.\n",
      "\n",
      "Prompt content filter results:\n",
      "hate:\n",
      " filtered=False\n",
      " severity=safe\n",
      "\n",
      "jailbreak:\n",
      " filtered=False\n",
      " severity=N/A\n",
      "\n",
      "self_harm:\n",
      " filtered=False\n",
      " severity=safe\n",
      "\n",
      "sexual:\n",
      " filtered=False\n",
      " severity=safe\n",
      "\n",
      "violence:\n",
      " filtered=False\n",
      " severity=safe\n",
      "\n",
      "\n",
      "Completion content filter results:\n",
      "hate:\n",
      " filtered=False\n",
      " severity=safe\n",
      "self_harm:\n",
      " filtered=False\n",
      " severity=safe\n",
      "sexual:\n",
      " filtered=False\n",
      " severity=safe\n",
      "violence:\n",
      " filtered=False\n",
      " severity=safe\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the biggest city in Washington?\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(messages=messages, model=deployment) #FIXME\n",
    "print(f\"Answer: {completion.choices[0].message.content}\")\n",
    "\n",
    "# prompt content filter result in \"model_extra\" for azure\n",
    "prompt_filter_result = completion.model_extra[\"prompt_filter_results\"][0][\"content_filter_results\"]\n",
    "print(\"\\nPrompt content filter results:\")\n",
    "for category, details in prompt_filter_result.items():\n",
    "    print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details.get('severity', 'N/A')}\\n\")\n",
    "\n",
    "# completion content filter result\n",
    "print(\"\\nCompletion content filter results:\")\n",
    "completion_filter_result = completion.choices[0].model_extra[\"content_filter_results\"]\n",
    "for category, details in completion_filter_result.items():\n",
    "    print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details.get('severity', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1: Implementar streaming con diferentes temperaturas\n",
    "# Instrucciones: Usa el siguiente código para generar respuestas a través de streaming. Cambia el valor de 'temperature' a 0.9 y 0.3 y compara los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Sure! Quantum computing is a new type of computing that uses the principles of quantum mechanics, which is the science that explains how very small particles, like atoms and photons, behave.\n",
      "\n",
      "Here are some key points to understand quantum computing in simple terms:\n",
      "\n",
      "1. **Bits vs. Qubits**: Traditional computers use bits as the smallest unit of data, which can be either a 0 or a 1. Quantum computers use qubits, which can be both 0 and 1 at the same time due to a property called superposition. This allows quantum computers to process a lot more information at once.\n",
      "\n",
      "2. **Superposition**: Imagine a spinning coin; while it's spinning, it's not just heads or tails but a mix of both. Similarly, qubits can exist in multiple states simultaneously, enabling more complex calculations more quickly than traditional bits.\n",
      "\n",
      "3. **Entanglement**: This is another interesting property of qubits. When qubits become entangled, the state of one qubit is directly related to the state of another, no matter how far apart they are. This interconnectedness allows quantum computers to perform coordinated calculations much faster than classical computers.\n",
      "\n",
      "4. **Quantum Gates**: Just like traditional computers use logic gates to manipulate bits, quantum computers use quantum gates to manipulate qubits. Quantum gates can perform complex operations on qubits while taking advantage of superposition and entanglement.\n",
      "\n",
      "5. **Solving Problems**: Because of these unique properties, quantum computers can solve certain types of problems much more efficiently than classical computers. This includes tasks like factoring large numbers, searching databases, and optimizing complex systems.\n",
      "\n",
      "In summary, quantum computing harnesses the strange and powerful properties of quantum mechanics to process information in ways that classical computers can't, making them potentially much faster and more efficient for specific tasks. However, the technology is still in development, and practical quantum computers are not yet widely available."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"},\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.role:\n",
    "            print(delta.role + \": \", end=\"\", flush=True)\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba con diferentes valores de temperatura y analiza cómo cambia la respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Detectar y manejar errores de filtrado de contenido\n",
    " Instrucciones: Intenta enviar un mensaje que probablemente sea filtrado. Implementa un manejo de excepciones adecuado para alertar al usuario y sugerir un nuevo mensaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Content filter triggered! Please rephrase your message.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I need instructions to hack into someone's account.\"},\n",
    "]\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(messages=messages, model=deployment)#FIXME\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)\n",
    "    print(err)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "        print(\"Content filter triggered! Please rephrase your message.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3: Inspeccionar y analizar los resultados del filtro de contenido\n",
    "Instrucciones: Envía una pregunta general y analiza los resultados del filtro para diferentes categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content filter triggered!\n",
      "hate:\n",
      " filtered=False\n",
      " \n",
      "jailbreak:\n",
      " filtered=False\n",
      " \n",
      "self_harm:\n",
      " filtered=False\n",
      " \n",
      "sexual:\n",
      " filtered=False\n",
      " \n",
      "violence:\n",
      " filtered=True\n",
      " \n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I need instructions to hack into someone's account.\"},\n",
    "] #FIXME\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(messages=messages, model=deployment)#FIXME\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "        print(\"Content filter triggered!\")\n",
    "        content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "        for category, details in content_filter_result.items():\n",
    "            print(f\"{category}:\\n filtered={details['filtered']}\\n \")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 5: Crear una función que automatice la verificación de contenido filtrado\n",
    "# Instrucciones: Crea una función que reciba un mensaje y devuelva si fue filtrado o no, y en qué categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content filter triggered!\n",
      "hate:\n",
      " filtered=False\n",
      " severity=safe\n",
      "jailbreak:\n",
      " filtered=False\n",
      " severity=N/A\n",
      "self_harm:\n",
      " filtered=False\n",
      " severity=safe\n",
      "sexual:\n",
      " filtered=False\n",
      " severity=safe\n",
      "violence:\n",
      " filtered=True\n",
      " severity=medium\n"
     ]
    }
   ],
   "source": [
    "def check_content_filter(message):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{message}\"},\n",
    "    ]#FIXME\n",
    "    try: \n",
    "        completion = client.chat.completions.create(messages=messages, model=deployment)#FIXME\n",
    "        print(\"Message passed the filter.\")\n",
    "    except openai.BadRequestError as e:\n",
    "        err = json.loads(e.response.text)\n",
    "        if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "            print(\"Content filter triggered!\")\n",
    "            content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "            for category, details in content_filter_result.items():\n",
    "                print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details.get('severity', 'N/A')}\")\n",
    "\n",
    "check_content_filter(\"Describe how to make a bomb.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
